{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom glob import glob","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **image Preprocess**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"images_path = '../input/flickr8k-sau/Flickr_Data/Images/'\nimages = glob(images_path+'*.jpg')\nlen(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor i in range(5):\n    plt.figure()\n    img = cv2.imread(images[i])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import ResNet50\n\nincept_model = ResNet50(include_top=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nlast = incept_model.layers[-2].output\nmodele = Model(inputs = incept_model.input,outputs = last)\nmodele.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images_features = {}\ncount = 0\nfor i in images:\n    img = cv2.imread(i)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (224,224))\n    \n    img = img.reshape(1,224,224,3)\n    pred = modele.predict(img).reshape(2048,)\n        \n    img_name = i.split('/')[-1]\n    \n    images_features[img_name] = pred\n    \n    count += 1\n    \n    if count > 1499:\n        break\n        \n    elif count % 50 == 0:\n        print(count)\n    \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(images_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Text Preprocess**"},{"metadata":{"trusted":true},"cell_type":"code","source":"caption_path = '../input/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"captions = open(caption_path, 'rb').read().decode('utf-8').split('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(captions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"captions_dict = {}\nfor i in captions:\n    try:\n        img_name = i.split('\\t')[0][:-2] \n        caption = i.split('\\t')[1]\n        if img_name in images_features:\n            if img_name not in captions_dict:\n                captions_dict[img_name] = [caption]\n                \n            else:\n                captions_dict[img_name].append(caption)\n            \n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(captions_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualize Images with captions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor i in range(5):\n    plt.figure()\n    img_name = images[i]\n    \n    \n    img = cv2.imread(img_name)\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.xlabel(captions_dict[img_name.split('/')[-1]])\n    plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor k in images_features.keys():\n    plt.figure()\n    \n    img_name = '../input/flickr8k-sau/Flickr_Data/Images/' + k\n    \n    \n    img = cv2.imread(img_name)\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.xlabel(captions_dict[img_name.split('/')[-1]])\n    plt.imshow(img)\n    \n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef preprocessed(txt):\n    modified = txt.lower()\n    modified = 'startofseq ' + modified + ' endofseq'\n    return modified\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,v in captions_dict.items():\n    for vv in v:\n        captions_dict[k][v.index(vv)] = preprocessed(vv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Create Vocabulary**"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_words = {}\nfor k,vv in captions_dict.items():\n    for v in vv:\n        for word in v.split():\n            if word not in count_words:\n\n                count_words[word] = 0\n\n            else:\n                count_words[word] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(count_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESH = -1\ncount = 1\nnew_dict = {}\nfor k,v in count_words.items():\n    if count_words[k] > THRESH:\n        new_dict[k] = count\n        count += 1\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(new_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dict['<OUT>'] = len(new_dict) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"captions_backup = captions_dict.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"captions_dict = captions_backup.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k, vv in captions_dict.items():\n    for v in vv:\n        encoded = []\n        for word in v.split():  \n            if word not in new_dict:\n                encoded.append(new_dict['<OUT>'])\n            else:\n                encoded.append(new_dict[word])\n\n\n        captions_dict[k][vv.index(v)] = encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"captions_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"# **Build Generator Function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 0\nfor k, vv in captions_dict.items():\n    for v in vv:\n        if len(v) > MAX_LEN:\n            MAX_LEN = len(v)\n            print(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"captions_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Batch_size = 5000\nVOCAB_SIZE = len(new_dict)\n\ndef generator(photo, caption):\n    n_samples = 0\n    \n    X = []\n    y_in = []\n    y_out = []\n    \n    for k, vv in caption.items():\n        for v in vv:\n            for i in range(1, len(v)):\n                X.append(photo[k])\n\n                in_seq= [v[:i]]\n                out_seq = v[i]\n\n                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]\n                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n\n                y_in.append(in_seq)\n                y_out.append(out_seq)\n            \n    return X, y_in, y_out\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y_in, y_out = generator(images_features, captions_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X), len(y_in), len(y_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(X)\ny_in = np.array(y_in, dtype='float64')\ny_out = np.array(y_out, dtype='float64')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y_in.shape, y_out.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[1510]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_in[2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom keras.models import Sequential, Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size = 128\nmax_len = MAX_LEN\nvocab_size = len(new_dict)\n\nimage_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()\n\nlanguage_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()\n\nconca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"../input/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([X, y_in], y_out, batch_size=512, epochs=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_dict = {v:k for k, v in new_dict.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('mine_model_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('vocab.npy', new_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getImage(x):\n    \n    test_img_path = images[x]\n\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n    test_img = cv2.resize(test_img, (299,299))\n\n    test_img = np.reshape(test_img, (1,299,299,3))\n    \n    return test_img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    \n    no = np.random.randint(1500,7000,(1,1))[0,0]\n    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n    \n    test_img_path = images[no]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n\n    text_inp = ['startofseq']\n\n    count = 0\n    caption = ''\n    while count < 25:\n        count += 1\n\n        encoded = []\n        for i in text_inp:\n            encoded.append(new_dict[i])\n\n        encoded = [encoded]\n\n        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n\n\n        prediction = np.argmax(model.predict([test_feature, encoded]))\n\n        sampled_word = inv_dict[prediction]\n\n        caption = caption + ' ' + sampled_word\n            \n        if sampled_word == 'endofseq':\n            break\n\n        text_inp.append(sampled_word)\n        \n    plt.figure()\n    plt.imshow(test_img)\n    plt.xlabel(caption)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}